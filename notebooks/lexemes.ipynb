{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# UD_Dutch\n",
    "data_dir = '/home/jvdzwaan/data/UD_Dutch/'\n",
    "test = data_dir + 'nl-ud-test.conllu'\n",
    "train = data_dir + 'nl-ud-train.conllu'\n",
    "\n",
    "# UD_Dutch_LassySmall\n",
    "#data_dir = '/home/jvdzwaan/data/UD_Dutch-LassySmall/'\n",
    "#test = data_dir + 'nl_lassysmall-ud-test.conllu'\n",
    "#train = data_dir + 'nl_lassysmall-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence(data):\n",
    "    words = []\n",
    "    tags = []\n",
    "    parts = data.split('\\n')\n",
    "    if parts:\n",
    "        for part in parts:\n",
    "            if not part.startswith('#'):\n",
    "                d = part.split()\n",
    "                if len(d) > 3:\n",
    "                    words.append(d[1])\n",
    "                    tags.append(d[3])\n",
    "                \n",
    "    return words, tags\n",
    "    \n",
    "\n",
    "def read_connl(filepath):\n",
    "    with codecs.open(filepath, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    sentences = data.split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        yield get_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w, t in read_connl(train):\n",
    "    text = ' '.join(w)\n",
    "    doc = nlp(unicode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lexeme.Lexeme object at 0x7f4c6bb413c0>\n"
     ]
    }
   ],
   "source": [
    "#access lexemes\n",
    "l = nlp.vocab[u'boek']\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'check_flag',\n",
       " 'cluster',\n",
       " 'flags',\n",
       " 'has_vector',\n",
       " 'is_alpha',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'repvec',\n",
       " 'sentiment',\n",
       " 'set_flag',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8214L"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "words =[u'werken', u'muis', u'chocolade', u'reeds', u'insect', u'tuinman']\n",
    "for word in words:\n",
    "    l = nlp.vocab[word]\n",
    "    print l.cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
