{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from spacy.munge import read_conll\n",
    "\n",
    "data_dir = '/home/jvdzwaan/data/UD_Dutch/'\n",
    "\n",
    "with codecs.open(data_dir+'nl-ud-train.conllu', 'rb', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence(data):\n",
    "    words = []\n",
    "    tags = []\n",
    "    parts = data.split('\\n')\n",
    "    if parts:\n",
    "        for part in parts:\n",
    "            if not part.startswith('#'):\n",
    "                d = part.split()\n",
    "                if len(d) > 3:\n",
    "                    words.append(d[1])\n",
    "                    tags.append(d[3])\n",
    "                \n",
    "    return words, tags\n",
    "    \n",
    "\n",
    "def read_connl(filepath):\n",
    "    with codecs.open(filepath, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    sentences = data.split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        yield get_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'ADV': {'pos': u'ADV'}, u'NOUN': {'pos': u'NOUN'}, u'ADP': {'pos': u'ADP'}, u'PRON': {'pos': u'PRON'}, u'SCONJ': {'pos': u'SCONJ'}, u'PROPN': {'pos': u'PROPN'}, u'DET': {'pos': u'DET'}, u'SYM': {'pos': u'SYM'}, u'INTJ': {'pos': u'INTJ'}, u'PUNCT': {'pos': u'PUNCT'}, u'NUM': {'pos': u'NUM'}, u'AUX': {'pos': u'AUX'}, u'X': {'pos': u'X'}, u'CONJ': {'pos': u'CONJ'}, u'ADJ': {'pos': u'ADJ'}, u'VERB': {'pos': u'VERB'}}\n"
     ]
    }
   ],
   "source": [
    "# create tag map\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "tags = set()\n",
    "data = []\n",
    "for w, t in read_connl(data_dir+'nl-ud-train.conllu'):\n",
    "    tags.update(t)\n",
    "    data.append((w,t))\n",
    "\n",
    "tag_map = {}    \n",
    "for t in tags:\n",
    "    tag_map[t] = {'pos': t}\n",
    "print tag_map\n",
    "\n",
    "with codecs.open('/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/tag_map.json', 'wb', encoding='utf-8') as f:\n",
    "    json.dump(tag_map, f, indent=2, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'ADV': {u'pos': u'ADV'}, u'NOUN': {u'pos': u'NOUN'}, u'ADP': {u'pos': u'ADP'}, u'PUNCT': {u'pos': u'PUNCT'}, u'SCONJ': {u'pos': u'SCONJ'}, u'PROPN': {u'pos': u'PROPN'}, u'DET': {u'pos': u'DET'}, u'SYM': {u'pos': u'SYM'}, u'INTJ': {u'pos': u'INTJ'}, u'PRON': {u'pos': u'PRON'}, u'NUM': {u'pos': u'NUM'}, u'X': {u'pos': u'X'}, u'AUX': {u'pos': u'AUX'}, u'CONJ': {u'pos': u'CONJ'}, u'ADJ': {u'pos': u'ADJ'}, u'VERB': {u'pos': u'VERB'}}\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/tag_map.json', encoding='utf-8') as f:\n",
    "    tag_map = json.load(f, encoding='utf-8')\n",
    "print tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "number of sentences 13001\n",
      "number of sentences that failed 0\n"
     ]
    }
   ],
   "source": [
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "import random\n",
    "\n",
    "vocab = Vocab(tag_map=tag_map)\n",
    "tagger = Tagger(vocab)\n",
    "\n",
    "n_iter = 25\n",
    "failed_sentences = 0\n",
    "sentences = 0\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print i,\n",
    "    random.shuffle(data)\n",
    "    for sample in data:\n",
    "        sentences += 1\n",
    "            \n",
    "        try:\n",
    "            doc = Doc(vocab, words=sample[0])\n",
    "            gold = GoldParse(doc, tags=sample[1])\n",
    "            \n",
    "            tagger.update(doc, gold)\n",
    "        except Exception:\n",
    "             failed_sentences += 1\n",
    "\n",
    "print\n",
    "print 'number of sentences', sentences/n_iter\n",
    "print 'number of sentences that failed', failed_sentences/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger.model.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/\n",
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/\n",
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/pos/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_dirs(fname):\n",
    "    \"\"\"Create (output) directories if they don't exist\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fname):\n",
    "        os.makedirs(fname)\n",
    "        \n",
    "out_dirs = ['/home/jvdzwaan/data/tmp/sherlock/spaCy/', '/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/', '/home/jvdzwaan/data/tmp/sherlock/spaCy/pos/']\n",
    "for o in out_dirs:\n",
    "    print o\n",
    "    create_dirs(o)\n",
    "\n",
    "tagger.model.dump(os.path.join(out_dirs[0], 'pos', 'model'))\n",
    "with codecs.open(os.path.join(out_dirs[0], 'vocab', 'strings.json'), 'wb', encoding='utf-8') as file_:\n",
    "    tagger.vocab.strings.dump(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# save other relevant models (taken from spaCy/language.py end_training line 353)\n",
    "from spacy.attrs import TAG\n",
    "tagger_freqs = list(tagger.freqs[TAG].items())\n",
    "print len(tagger_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(os.path.join(out_dirs[0], 'vocab', 'serializer.json'), 'wb', encoding='utf-8') as file_:\n",
    "            file_.write(\n",
    "                json.dumps([\n",
    "                    (TAG, tagger_freqs)\n",
    "                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab.dump(os.path.join(out_dirs[0], 'vocab', 'lexemes.bin'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
