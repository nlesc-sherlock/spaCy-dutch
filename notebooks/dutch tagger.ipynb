{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "# sources:\n",
    "# https://spacy.io/docs/usage/training\n",
    "# https://github.com/explosion/spaCy/blob/master/examples/training/train_tagger.py\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "import random\n",
    "\n",
    "# You need to define a mapping from your data's part-of-speech tag names to the\n",
    "# Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.\n",
    "# See here for the Universal Tag Set:\n",
    "# http://universaldependencies.github.io/docs/u/pos/index.html\n",
    "# You may also specify morphological features for your tags, from the universal\n",
    "# scheme.\n",
    "with codecs.open('/home/jvdzwaan/data/tmp/sherlock/nl_tag_map.json', 'rb', encoding='utf-8') as f:\n",
    "    TAG_MAP = json.load(f)\n",
    "\n",
    "print len(TAG_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "vocab = Vocab(tag_map=TAG_MAP)\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create tagger\n",
    "\n",
    "# The default_templates argument is where features are specified. See\n",
    "# spacy/tagger.pyx for the defaults.\n",
    "tagger = Tagger(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 files\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n",
      "write to file\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "# data is stored in json files of approximately 1 Mb\n",
    "# data format [(['list' 'of', 'words'], ['list' 'of' 'tags']), ..]\n",
    "\n",
    "import glob\n",
    "import uuid\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Usually you'll read this in, of course. Data formats vary.\n",
    "# Ensure your strings are unicode\n",
    "files = glob.glob('/home/jvdzwaan/data/tmp/sherlock/nlwiki-json/*.json')\n",
    "print len(files), 'files'\n",
    "\n",
    "data = []\n",
    "SIZE = 1 * 1000 * 1000\n",
    "\n",
    "def write_to_file(data):\n",
    "    fname = '{}.json'.format(str(uuid.uuid4()))\n",
    "    with codecs.open(os.path.join('/home/jvdzwaan/data/tmp/sherlock/tagger-train/', fname), 'wb', encoding='utf-8') as f:\n",
    "        json.dump(data, f, encoding='utf-8')\n",
    "    \n",
    "\n",
    "for fi in files:\n",
    "    with codecs.open(fi, 'rb', encoding='utf-8') as f:\n",
    "        saf = json.load(f)\n",
    "    #print len(saf['tokens'])\n",
    "    tokens = saf['tokens']\n",
    "    sent = tokens[0]['sentence']\n",
    "    s = []\n",
    "    tags = []\n",
    "    #print sent\n",
    "    for token in tokens:\n",
    "        if token['sentence'] != sent:\n",
    "            # save s and tags\n",
    "            data.append((s, tags))\n",
    "            \n",
    "            #print sys.getsizeof(json.dumps(data)), len(data)\n",
    "            if sys.getsizeof(json.dumps(data)) >= SIZE:\n",
    "                print 'write to file'\n",
    "                write_to_file(data)\n",
    "                \n",
    "                data = []\n",
    "             \n",
    "            # reset s and tags\n",
    "            s = []\n",
    "            tags = []\n",
    "\n",
    "            sent = token['sentence']\n",
    "        \n",
    "        words = token['word'].split('_')\n",
    "        pos_tags = token['pos'].split('_')\n",
    "        for w, p in zip(words, pos_tags):\n",
    "            s.append(w)\n",
    "            tags.append(p)\n",
    "write_to_file(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 files\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "number of sentences 40303\n",
      "number of sentences that failed 5\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('/home/jvdzwaan/data/tmp/sherlock/tagger-train/*.json')\n",
    "print len(files), 'files'\n",
    "\n",
    "n_iter = 25\n",
    "failed_sentences = 0\n",
    "sentences = 0\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print i,\n",
    "    random.shuffle(files)\n",
    "    for fi in files:\n",
    "        with codecs.open(fi, 'rb', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        random.shuffle(data)\n",
    "        for sample in data:          \n",
    "            sentences += 1\n",
    "            \n",
    "            try:\n",
    "                doc = Doc(vocab, words=sample[0])\n",
    "                gold = GoldParse(doc, tags=sample[1])\n",
    "            \n",
    "                tagger.update(doc, gold)\n",
    "            except Exception:\n",
    "                failed_sentences += 1\n",
    "\n",
    "print\n",
    "print 'number of sentences', sentences/n_iter\n",
    "print 'number of sentences that failed', failed_sentences/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger.model.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tagger.TaggerModel"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tagger.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/\n",
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/\n",
      "/home/jvdzwaan/data/tmp/sherlock/spaCy/pos/\n"
     ]
    }
   ],
   "source": [
    "def create_dirs(fname):\n",
    "    \"\"\"Create (output) directories if they don't exist\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fname):\n",
    "        os.makedirs(fname)\n",
    "        \n",
    "out_dirs = ['/home/jvdzwaan/data/tmp/sherlock/spaCy/', '/home/jvdzwaan/data/tmp/sherlock/spaCy/vocab/', '/home/jvdzwaan/data/tmp/sherlock/spaCy/pos/']\n",
    "for o in out_dirs:\n",
    "    print o\n",
    "    create_dirs(o)\n",
    "\n",
    "tagger.model.dump(os.path.join(out_dirs[0], 'pos', 'model'))\n",
    "with codecs.open(os.path.join(out_dirs[0], 'vocab', 'strings.json'), 'wb', encoding='utf-8') as file_:\n",
    "    tagger.vocab.strings.dump(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "# save other relevant models (taken from spaCy/language.py end_training line 353)\n",
    "from spacy.attrs import TAG\n",
    "tagger_freqs = list(tagger.freqs[TAG].items())\n",
    "print len(tagger_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(os.path.join(out_dirs[0], 'vocab', 'serializer.json'), 'wb', encoding='utf-8') as file_:\n",
    "            file_.write(\n",
    "                json.dumps([\n",
    "                    (TAG, tagger_freqs)\n",
    "                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.dump(os.path.join(out_dirs[0], 'vocab', 'lexemes.bin'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
